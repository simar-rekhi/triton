{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simar-rekhi/triton/blob/main/Neel_Triton_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "625d432b",
      "metadata": {
        "id": "625d432b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade6a32c-45fa-42b9-99d5-e1a22eda0141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "E8QfJH8cDAEN",
        "outputId": "871f1cd3-45f0-47c5-fef9-8e1580fdb1fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "E8QfJH8cDAEN",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4662b919",
      "metadata": {
        "id": "4662b919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f63ee50-eba4-43ef-9137-44f445909a3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch, triton\n",
        "print(torch.cuda.is_available())\n",
        "# it has to be True, triton does not support CPU!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing operation alpha * A + B = C, where A and B are vectors and alpha is a constant.\n",
        "#Attempt 1\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def add_scaled_vector_kernel(A, B, C, N, alpha, BLOCK_SIZE: tl.constexpr):\n",
        "    pid = tl.program_id(0)\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < N\n",
        "    a = tl.load(A + offsets, mask=mask)\n",
        "    b = tl.load(B + offsets, mask=mask)\n",
        "    a = a * alpha\n",
        "    c = a + b\n",
        "    tl.store(C + offsets, c, mask=mask)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) The Triton kernel itself\n",
        "# ---------------------------\n",
        "\n",
        "# ---------------------------------\n",
        "# 2) A small helper for benchmarking\n",
        "# ---------------------------------\n",
        "def time_op_gpu(fn, sync=True, warmup=5, iters=20):\n",
        "    \"\"\"\n",
        "    Time a GPU operation using CUDA events for better accuracy (no CPU scheduling noise).\n",
        "    - fn: a callable that launches GPU work\n",
        "    - sync: whether to synchronize after each iteration (True recommended)\n",
        "    - warmup: warm-up iterations to let JIT/caches settle\n",
        "    - iters: timed iterations\n",
        "\n",
        "    Returns: average time in milliseconds over 'iters' runs.\n",
        "    \"\"\"\n",
        "    # warm-up does JIT and warms caches\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    if sync:\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    elapsed_ms = 0.0\n",
        "    for _ in range(iters):\n",
        "        start.record()\n",
        "        fn()\n",
        "        end.record()\n",
        "        # Wait for the events to be recorded & measure GPU time\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed_ms += start.elapsed_time(end)\n",
        "    return elapsed_ms / iters\n",
        "\n",
        "\n",
        "# ---------------\n",
        "# 3) Driver code\n",
        "# ---------------\n",
        "def main():\n",
        "    assert torch.cuda.is_available(), \"CUDA device not found. Please run on a machine with an NVIDIA GPU.\"\n",
        "    print(\"Testing vector addition with scaled vector:\")\n",
        "    N = 1 << 24\n",
        "\n",
        "\n",
        "    a = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n",
        "    b = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n",
        "    c = torch.empty_like(a)\n",
        "    alpha = 3\n",
        "\n",
        "    BLOCK_SIZE = 1024\n",
        "\n",
        "    grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE\"]),)\n",
        "\n",
        "    add_scaled_vector_kernel[grid](a, b, c, N, alpha, BLOCK_SIZE=BLOCK_SIZE)\n",
        "    torch.cuda.synchronize()\n",
        "    ok = torch.allclose(c, a + b, rtol=1e-5, atol=1e-6)\n",
        "    print(f\"[Correctness] Triton result matches PyTorch: {ok}\")\n",
        "\n",
        "\n",
        "    def launch_triton():\n",
        "        add_scaled_vector_kernel[grid](a, b, c, N, alpha, BLOCK_SIZE=BLOCK_SIZE)\n",
        "\n",
        "    def launch_torch():\n",
        "        c.copy_(a + b)\n",
        "\n",
        "    triton_ms = time_op_gpu(launch_triton)\n",
        "    torch_ms = time_op_gpu(launch_torch)\n",
        "\n",
        "\n",
        "    bytes_moved = N * 12\n",
        "    triton_bw = bytes_moved / (triton_ms / 1e3) / 1e9  # GB/s\n",
        "    torch_bw = bytes_moved / (torch_ms / 1e3) / 1e9    # GB/s\n",
        "\n",
        "    print(f\"[Perf] Triton: {triton_ms:.3f} ms  (~{triton_bw:.1f} GB/s)\")\n",
        "    print(f\"[Perf] PyTorch: {torch_ms:.3f} ms  (~{torch_bw:.1f} GB/s)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rLqlcMeHEcjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d19d598-4af1-404d-82d4-d79e26d386cc"
      },
      "id": "rLqlcMeHEcjF",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing vector addition with scaled vector:\n",
            "[Correctness] Triton result matches PyTorch: False\n",
            "[Perf] Triton: 0.894 ms  (~225.3 GB/s)\n",
            "[Perf] PyTorch: 1.444 ms  (~139.4 GB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "19cd6ec7",
      "metadata": {
        "id": "19cd6ec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a58028-4737-4844-d910-d475184e658a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing vector addition with scaled vector:\n",
            "[Correctness] Triton result matches PyTorch: True\n",
            "[Perf] Triton: 0.893 ms  (~225.5 GB/s)\n",
            "[Perf] PyTorch: 2.007 ms  (~100.3 GB/s)\n"
          ]
        }
      ],
      "source": [
        "#Performing operation alpha * A + B = C, where A and B are vectors and alpha is a constant.\n",
        "#Attempt 2\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def add_scaled_vector_kernel(A, B, C, N, alpha, BLOCK_SIZE: tl.constexpr):\n",
        "    pid = tl.program_id(0)\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < N\n",
        "    a = tl.load(A + offsets, mask=mask)\n",
        "    b = tl.load(B + offsets, mask=mask)\n",
        "    a = a * alpha\n",
        "    c = a + b\n",
        "    tl.store(C + offsets, c, mask=mask)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) The Triton kernel itself\n",
        "# ---------------------------\n",
        "\n",
        "# ---------------------------------\n",
        "# 2) A small helper for benchmarking\n",
        "# ---------------------------------\n",
        "def time_op_gpu(fn, sync=True, warmup=5, iters=20):\n",
        "    \"\"\"\n",
        "    Time a GPU operation using CUDA events for better accuracy (no CPU scheduling noise).\n",
        "    - fn: a callable that launches GPU work\n",
        "    - sync: whether to synchronize after each iteration (True recommended)\n",
        "    - warmup: warm-up iterations to let JIT/caches settle\n",
        "    - iters: timed iterations\n",
        "\n",
        "    Returns: average time in milliseconds over 'iters' runs.\n",
        "    \"\"\"\n",
        "    # warm-up does JIT and warms caches\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    if sync:\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    elapsed_ms = 0.0\n",
        "    for _ in range(iters):\n",
        "        start.record()\n",
        "        fn()\n",
        "        end.record()\n",
        "        # Wait for the events to be recorded & measure GPU time\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed_ms += start.elapsed_time(end)\n",
        "    return elapsed_ms / iters\n",
        "\n",
        "\n",
        "# ---------------\n",
        "# 3) Driver code\n",
        "# ---------------\n",
        "def main():\n",
        "    assert torch.cuda.is_available(), \"CUDA device not found. Please run on a machine with an NVIDIA GPU.\"\n",
        "    print(\"Testing vector addition with scaled vector:\")\n",
        "    N = 1 << 24\n",
        "\n",
        "\n",
        "    a = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n",
        "    b = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n",
        "    c = torch.empty_like(a)\n",
        "    alpha = 3\n",
        "\n",
        "    BLOCK_SIZE = 1024\n",
        "\n",
        "    grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE\"]),)\n",
        "\n",
        "    add_scaled_vector_kernel[grid](a, b, c, N, alpha, BLOCK_SIZE=BLOCK_SIZE)\n",
        "    torch.cuda.synchronize()\n",
        "    ok = torch.allclose(c, a * alpha + b, rtol=1e-5, atol=1e-6)\n",
        "    print(f\"[Correctness] Triton result matches PyTorch: {ok}\")\n",
        "\n",
        "\n",
        "    def launch_triton():\n",
        "        add_scaled_vector_kernel[grid](a, b, c, N, alpha, BLOCK_SIZE=BLOCK_SIZE)\n",
        "\n",
        "    def launch_torch():\n",
        "        c.copy_(a * alpha + b)\n",
        "\n",
        "    triton_ms = time_op_gpu(launch_triton)\n",
        "    torch_ms = time_op_gpu(launch_torch)\n",
        "\n",
        "\n",
        "    bytes_moved = N * 12\n",
        "    triton_bw = bytes_moved / (triton_ms / 1e3) / 1e9  # GB/s\n",
        "    torch_bw = bytes_moved / (torch_ms / 1e3) / 1e9    # GB/s\n",
        "\n",
        "    print(f\"[Perf] Triton: {triton_ms:.3f} ms  (~{triton_bw:.1f} GB/s)\")\n",
        "    print(f\"[Perf] PyTorch: {torch_ms:.3f} ms  (~{torch_bw:.1f} GB/s)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "69fa8d6d",
      "metadata": {
        "id": "69fa8d6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93f4206-f0d0-42f9-a860-e5184fb7a479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing element-wise matrix multiplication:\n",
            "[Correctness] Triton result matches PyTorch: True\n",
            "[Perf] Triton: 0.889 ms  (~226.5 GB/s)\n",
            "[Perf] PyTorch: 1.424 ms  (~141.3 GB/s)\n"
          ]
        }
      ],
      "source": [
        "#Performing element-wise matrix multiplication. Each element in A is multiplied by its corresponding value in B, and the result is stored in C.\n",
        "#Attempt 1\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def element_wise_matrix_mult_kernel(A, B, C, N, BLOCK_SIZE: tl.constexpr):\n",
        "    pid = tl.program_id(0)\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < N\n",
        "    a = tl.load(A + offsets, mask=mask)\n",
        "    b = tl.load(B + offsets, mask=mask)\n",
        "    c = a * b\n",
        "    tl.store(C + offsets, c, mask=mask)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) The Triton kernel itself\n",
        "# ---------------------------\n",
        "\n",
        "# ---------------------------------\n",
        "# 2) A small helper for benchmarking\n",
        "# ---------------------------------\n",
        "def time_op_gpu(fn, sync=True, warmup=5, iters=20):\n",
        "    \"\"\"\n",
        "    Time a GPU operation using CUDA events for better accuracy (no CPU scheduling noise).\n",
        "    - fn: a callable that launches GPU work\n",
        "    - sync: whether to synchronize after each iteration (True recommended)\n",
        "    - warmup: warm-up iterations to let JIT/caches settle\n",
        "    - iters: timed iterations\n",
        "\n",
        "    Returns: average time in milliseconds over 'iters' runs.\n",
        "    \"\"\"\n",
        "    # warm-up does JIT and warms caches\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    if sync:\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    elapsed_ms = 0.0\n",
        "    for _ in range(iters):\n",
        "        start.record()\n",
        "        fn()\n",
        "        end.record()\n",
        "        # Wait for the events to be recorded & measure GPU time\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed_ms += start.elapsed_time(end)\n",
        "    return elapsed_ms / iters\n",
        "\n",
        "\n",
        "# ---------------\n",
        "# 3) Driver code\n",
        "# ---------------\n",
        "def main():\n",
        "    assert torch.cuda.is_available(), \"CUDA device not found. Please run on a machine with an NVIDIA GPU.\"\n",
        "    print(\"Testing element-wise matrix multiplication:\")\n",
        "    N = 1 << 24\n",
        "\n",
        "\n",
        "    a = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n",
        "    b = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n",
        "    c = torch.empty_like(a)\n",
        "\n",
        "    BLOCK_SIZE = 1024\n",
        "\n",
        "    grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE\"]),)\n",
        "\n",
        "    element_wise_matrix_mult_kernel[grid](a, b, c, N, BLOCK_SIZE=BLOCK_SIZE)\n",
        "    torch.cuda.synchronize()\n",
        "    ok = torch.allclose(c, a * b, rtol=1e-5, atol=1e-6)\n",
        "    print(f\"[Correctness] Triton result matches PyTorch: {ok}\")\n",
        "\n",
        "\n",
        "    def launch_triton():\n",
        "        element_wise_matrix_mult_kernel[grid](a, b, c, N, BLOCK_SIZE=BLOCK_SIZE)\n",
        "\n",
        "    def launch_torch():\n",
        "        c.copy_(a * b)\n",
        "\n",
        "    triton_ms = time_op_gpu(launch_triton)\n",
        "    torch_ms = time_op_gpu(launch_torch)\n",
        "\n",
        "\n",
        "    bytes_moved = N * 12\n",
        "    triton_bw = bytes_moved / (triton_ms / 1e3) / 1e9  # GB/s\n",
        "    torch_bw = bytes_moved / (torch_ms / 1e3) / 1e9    # GB/s\n",
        "\n",
        "    print(f\"[Perf] Triton: {triton_ms:.3f} ms  (~{triton_bw:.1f} GB/s)\")\n",
        "    print(f\"[Perf] PyTorch: {torch_ms:.3f} ms  (~{torch_bw:.1f} GB/s)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}